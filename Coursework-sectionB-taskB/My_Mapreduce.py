from multiprocessing import Process
from cffi.backend_ctypes import xrange
import json
import os
import sys
"""
explanation:
:param input_dir: directory of the input files, taken from the default utils if not provided
:param output_dir: directory of the output files, taken from the default utils if not provided
:param n_mappers: number of mapper threads to use, taken from the default utils if not provided
:param n_reducers: number of reducer threads to use, taken from the default utils if not provided
:param clean: optional, if True temporary files are deleted, True by default.
"""

#  default directory for the temporary map files
default_map_dir = "temp_map_files"


def get_input_file(input_dir=None, csv=".csv"):
    if not (input_dir is None):
        return input_dir + "/AComp_Passenger_data" + csv
    return "input_files" + "/AComp_Passenger_data" + csv


def get_input_split_file(index, input_dir=None, csv=".csv"):
    if not (input_dir is None):
        return input_dir + "/data_" + str(index) + csv
    return "input_files" + "/data_" + str(index) + csv


def get_temp_map_file(index, reducer, output_dir=None, tmp=".tmp"):
    if not (output_dir is None):
        return output_dir + "/map_data_" + str(index) + "-" + str(reducer) + tmp
    return "output_files" + "/map_data_" + str(index) + "-" + str(reducer) + tmp


def get_output_file(index, output_dir=None, out=".out"):
    if not (output_dir is None):
        return output_dir + "/reduce_data_" + str(index) + out
    return "output_files" + "/reduce_data_" + str(index) + out


def get_output_join_file(output_dir=None, csv=".csv"):
    if not (output_dir is None):
        return output_dir + "/output" + csv
    return "output_files" + "/output" + csv


# write MapReduce prototype
class My_MapReduce(object):
    def __init__(self, input_dir="input_files",output_dir="output_files",
                 n_mappers=4, n_reducers=4,
                 clean=True):
        #  Mapreduce task must run on Linux system
        if not sys.platform.startswith('linux'):
            raise Exception("Mapreduce task must run on Linux system")

        self.input_dir = input_dir
        self.output_dir = output_dir
        self.n_mappers = n_mappers
        self.n_reducers = n_reducers
        self.clean = clean
        self.file_handler = FileSystem(get_input_file(self.input_dir), self.output_dir)
        # Default is to split the file using a line count-based calculation
        self.file_handler.split_file_line(self.n_mappers)

    def mapper(self, key, value):
        # generate key-value pairs.
        pass

    def reducer(self, key, values_list):
        # generate weight of keyword
        pass

    def check_position(self, key, position):
        #  using hashmap -> if the given key is within the calculated range of the corresponding Reducer
        return position == (hash(key) % self.n_reducers)

    # run mapper process
    def excute_mapper(self, index):
        input_split_file = open(get_input_split_file(index), "r")
        key = input_split_file.readline()
        value = input_split_file.read()
        input_split_file.close()
        if (self.clean):
            os.unlink(get_input_split_file(index))
        # Group the intermediate results generated by Map task by key and save the intermediate results of each
        # group in json format for subsequent Reduce functions to process
        for reducer_index in range(self.n_reducers):
            temp_map_file = open(get_temp_map_file(index, reducer_index), "w+")
            json.dump([(key, value) for (key, value) in self.mapper(key, value) if self.check_position(key, reducer_index)] , temp_map_file)
            temp_map_file.close()

    # run reduce process
    def excute_reducer(self, index):
        key_values_map = {}
        # Aggregate all the values corresponding to the key
        for mapper_index in range(self.n_mappers):
            temp_map_file = open(get_temp_map_file(mapper_index, index), "r")
            mapper_results = json.load(temp_map_file)
            for (key, value) in mapper_results:
                if not (key in key_values_map):
                    key_values_map[key] = []
                try:
                    key_values_map[key].append(value)
                except Exception as e:
                    print("Exception : " + str(e))
            temp_map_file.close()
            if self.clean:
                os.unlink(get_temp_map_file(mapper_index, index))
        key_value_list = []
        for key in key_values_map:
            key_value_list.append(self.reducer(key, key_values_map[key]))
        output_file = open(get_output_file(index), "w+")
        json.dump(key_value_list, output_file)
        output_file.close()

    def run(self, join=True):
        #  mappers list
        map_workers = []
        #  reducers list
        rdc_workers = []
        # map process
        for thread_id in range(self.n_mappers):
            p = Process(target=self.excute_mapper, args=(thread_id,))
            p.start()
            map_workers.append(p)
        [t.join() for t in map_workers]
        # reduce process
        for thread_id in range(self.n_reducers):
            p = Process(target=self.excute_reducer, args=(thread_id,))
            p.start()
            rdc_workers.append(p)
        [t.join() for t in rdc_workers]
        if join:
            self.join_outputs()

    def join_outputs(self, clean=True, sort=True, decreasing=True):

        try:
            return self.file_handler.join_files(self.n_reducers, clean, sort, decreasing)
        except Exception as e:
            print("Exception: " + str(e))
            return []


# Manages splitting input files and joining outputs together.
class FileSystem(object):

    # the input file path should be given for splitting.
    def __init__(self, input_file_path, output_dir):
        self.input_file_path = input_file_path
        self.output_dir = output_dir

    # Initialize a split file by opening and adding an index.
    def initiate_file_split(self, split_index, index):
        file_split = open(get_input_split_file(split_index - 1), "w+")
        file_split.write(str(index) + "\n")
        return file_split

    # identify if it is the right time to split.(Calculation based on number of characters)
    def is_on_split_position_char(self, character, index, split_size, current_split):
        return index > split_size * current_split + 1 and character.isspace()

    # identify if it is the right time to split.(Calculation based on number of lines)
    def is_on_split_position_line(self, index, split_size, current_split):
        return index > split_size * current_split + 1

    # Split a file into multiple files.(Calculation based on number of characters)
    def split_file_char(self, number_of_splits):
        file_size = os.path.getsize(self.input_file_path)
        unit_size = file_size / number_of_splits + 1
        original_file = open(self.input_file_path, "r")
        file_content = original_file.read()
        original_file.close()
        (index, current_split_index) = (1, 1)
        current_split_unit = self.initiate_file_split(current_split_index, index)
        for character in file_content:
            current_split_unit.write(character)
            if self.is_on_split_position_char(character, index, unit_size, current_split_index):
                current_split_unit.close()
                current_split_index += 1
                current_split_unit = self.initiate_file_split(current_split_index, index)
            index += 1
        current_split_unit.close()

    # Split a file into multiple files.(Calculation based on number of lines)
    def split_file_line(self, number_of_splits):
        with open(self.input_file_path, "r") as input_file:
            # Count num of strings
            line_count = sum(1 for line in input_file)
        unit_size = line_count // number_of_splits + 1
        (index, current_split_index) = (1, 1)
        current_split_unit = self.initiate_file_split(current_split_index, index)
        with open(self.input_file_path, "r") as input_file:
            for line in input_file:
                current_split_unit.write(line)
                if self.is_on_split_position_line(index, unit_size, current_split_index):
                    current_split_unit.close()
                    current_split_index += 1
                    current_split_unit = self.initiate_file_split(current_split_index, index)
                index += 1
            current_split_unit.close()

    # Join all the files in the output directory into a single output file.
    def join_files(self, number_of_files, clean=False, sort=True, decreasing=True):
        output_join_list = []
        for reducer_index in xrange(0, number_of_files):
            f = open(get_output_file(reducer_index), "r")
            output_join_list += json.load(f)
            f.close()
            if clean:
                os.unlink(get_output_file(reducer_index))
        if sort:
            from operator import itemgetter as operator_ig
            # sort using the key
            output_join_list.sort(key=operator_ig(1), reverse=decreasing)
        output_join_file = open(get_output_join_file(self.output_dir), "w+")
        for item in output_join_list:
            output_join_file.write('{},{}\n'.format(item[0], item[1]))
        output_join_file.close()
        return output_join_list
